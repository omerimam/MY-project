{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt # show graph\n",
    "\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from hmmlearn import hmm\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-20T21:46:46.597658Z",
     "iopub.status.busy": "2021-10-20T21:46:46.597406Z",
     "iopub.status.idle": "2021-10-20T21:46:47.840632Z",
     "shell.execute_reply": "2021-10-20T21:46:47.839703Z",
     "shell.execute_reply.started": "2021-10-20T21:46:46.597626Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      sentence           Word  POS Tag\n",
       "0  Sentence: 1      Thousands  NNS   O\n",
       "1  Sentence: 1             of   IN   O\n",
       "2  Sentence: 1  demonstrators  NNS   O\n",
       "3  Sentence: 1           have  VBP   O\n",
       "4  Sentence: 1        marched  VBN   O"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"/kaggle/input/name-entity-recognition-ner-dataset/NER dataset.csv\", encoding='latin1')\n",
    "data = data.fillna(method=\"ffill\")\n",
    "data = data.rename(columns={'Sentence #': 'sentence'})\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the numbers of tags & words inside the whole data. We'll need this in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-20T21:46:47.842792Z",
     "iopub.status.busy": "2021-10-20T21:46:47.842255Z",
     "iopub.status.idle": "2021-10-20T21:46:47.990971Z",
     "shell.execute_reply": "2021-10-20T21:46:47.990091Z",
     "shell.execute_reply.started": "2021-10-20T21:46:47.842733Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42, 35178)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags = list(set(data.POS.values)) #Read POS values\n",
    "words = list(set(data.Word.values))\n",
    "len(tags), len(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We cannot split data normally with `train_test_split` because doing that makes some parts of a sentence in the training set while some others in the testing set. Instead, we use `GroupShuffleSplit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-20T21:46:47.995700Z",
     "iopub.status.busy": "2021-10-20T21:46:47.994256Z",
     "iopub.status.idle": "2021-10-20T21:46:49.891110Z",
     "shell.execute_reply": "2021-10-20T21:46:49.890296Z",
     "shell.execute_reply.started": "2021-10-20T21:46:47.995654Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>Families</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>soldiers</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>killed</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>in</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048570</th>\n",
       "      <td>Sentence: 47959</td>\n",
       "      <td>they</td>\n",
       "      <td>PRP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048571</th>\n",
       "      <td>Sentence: 47959</td>\n",
       "      <td>responded</td>\n",
       "      <td>VBD</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048572</th>\n",
       "      <td>Sentence: 47959</td>\n",
       "      <td>to</td>\n",
       "      <td>TO</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048573</th>\n",
       "      <td>Sentence: 47959</td>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048574</th>\n",
       "      <td>Sentence: 47959</td>\n",
       "      <td>attack</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>702936 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                sentence       Word  POS Tag\n",
       "24           Sentence: 2   Families  NNS   O\n",
       "25           Sentence: 2         of   IN   O\n",
       "26           Sentence: 2   soldiers  NNS   O\n",
       "27           Sentence: 2     killed  VBN   O\n",
       "28           Sentence: 2         in   IN   O\n",
       "...                  ...        ...  ...  ..\n",
       "1048570  Sentence: 47959       they  PRP   O\n",
       "1048571  Sentence: 47959  responded  VBD   O\n",
       "1048572  Sentence: 47959         to   TO   O\n",
       "1048573  Sentence: 47959        the   DT   O\n",
       "1048574  Sentence: 47959     attack   NN   O\n",
       "\n",
       "[702936 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = data.POS\n",
    "X = data.drop('POS', axis=1)\n",
    "\n",
    "gs = GroupShuffleSplit(n_splits=2, test_size=.33, random_state=42)\n",
    "train_ix, test_ix = next(gs.split(X, y, groups=data['sentence']))\n",
    "\n",
    "data_train = data.loc[train_ix]\n",
    "data_test = data.loc[test_ix]\n",
    "\n",
    "data_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-20T20:11:20.970394Z",
     "iopub.status.busy": "2021-10-20T20:11:20.970165Z",
     "iopub.status.idle": "2021-10-20T20:11:20.975803Z",
     "shell.execute_reply": "2021-10-20T20:11:20.974565Z",
     "shell.execute_reply.started": "2021-10-20T20:11:20.970358Z"
    }
   },
   "source": [
    "After checking the data after splitted, it seems to be fine.\n",
    "Check the numbers of tags & words in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-20T21:46:49.893229Z",
     "iopub.status.busy": "2021-10-20T21:46:49.892689Z",
     "iopub.status.idle": "2021-10-20T21:46:50.015418Z",
     "shell.execute_reply": "2021-10-20T21:46:50.014596Z",
     "shell.execute_reply.started": "2021-10-20T21:46:49.893188Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42, 29587)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags = list(set(data_train.POS.values)) #Read POS values\n",
    "words = list(set(data_train.Word.values))\n",
    "len(tags), len(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of tags is enough but the number of words is not enough (~29k vs ~35k).\n",
    "Because of that we need to randomly add some UNKNOWN words into the training dataset then we recalculate the word list and create map from them to number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-20T21:46:50.016857Z",
     "iopub.status.busy": "2021-10-20T21:46:50.016583Z",
     "iopub.status.idle": "2021-10-20T21:46:50.410317Z",
     "shell.execute_reply": "2021-10-20T21:46:50.409744Z",
     "shell.execute_reply.started": "2021-10-20T21:46:50.016820Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42, 27554)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfupdate = data_train.sample(frac=.15, replace=False, random_state=42)\n",
    "dfupdate.Word = 'UNKNOWN'\n",
    "data_train.update(dfupdate)\n",
    "words = list(set(data_train.Word.values))\n",
    "# Convert words and tags into numbers\n",
    "word2id = {w: i for i, w in enumerate(words)}\n",
    "tag2id = {t: i for i, t in enumerate(tags)}\n",
    "id2tag = {i: t for i, t in enumerate(tags)}\n",
    "len(tags), len(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hidden Markov Models can be trained by using the Baum-Welch algorithm.\n",
    "However input of the training is just dataset (Words).\n",
    "We cannot map back the states to the POS tag.\n",
    "\n",
    "That's why we have to calculate the model parameters for `hmmlearn.hmm.MultinomialHMM` manually by calculating\n",
    "- `startprob_`\n",
    "- `transmat_`\n",
    "- `emissionprob_`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-20T21:46:50.412372Z",
     "iopub.status.busy": "2021-10-20T21:46:50.411567Z",
     "iopub.status.idle": "2021-10-20T21:46:52.901682Z",
     "shell.execute_reply": "2021-10-20T21:46:52.900710Z",
     "shell.execute_reply.started": "2021-10-20T21:46:50.412335Z"
    }
   },
   "outputs": [],
   "source": [
    "count_tags = dict(data_train.POS.value_counts())\n",
    "count_tags_to_words = data_train.groupby(['POS']).apply(lambda grp: grp.groupby('Word')['POS'].count().to_dict()).to_dict()\n",
    "count_init_tags = dict(data_train.groupby('sentence').first().POS.value_counts())\n",
    "\n",
    "# TODO use panda solution\n",
    "count_tags_to_next_tags = np.zeros((len(tags), len(tags)), dtype=int)\n",
    "sentences = list(data_train.sentence)\n",
    "pos = list(data_train.POS)\n",
    "for i in range(len(sentences)) :\n",
    "    if (i > 0) and (sentences[i] == sentences[i - 1]):\n",
    "        prevtagid = tag2id[pos[i - 1]]\n",
    "        nexttagid = tag2id[pos[i]]\n",
    "        count_tags_to_next_tags[prevtagid][nexttagid] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-20T21:46:52.903251Z",
     "iopub.status.busy": "2021-10-20T21:46:52.902940Z",
     "iopub.status.idle": "2021-10-20T21:46:54.236735Z",
     "shell.execute_reply": "2021-10-20T21:46:54.235733Z",
     "shell.execute_reply.started": "2021-10-20T21:46:52.903216Z"
    }
   },
   "outputs": [],
   "source": [
    "mystartprob = np.zeros((len(tags),))\n",
    "mytransmat = np.zeros((len(tags), len(tags)))\n",
    "myemissionprob = np.zeros((len(tags), len(words)))\n",
    "num_sentences = sum(count_init_tags.values())\n",
    "sum_tags_to_next_tags = np.sum(count_tags_to_next_tags, axis=1)\n",
    "for tag, tagid in tag2id.items():\n",
    "    floatCountTag = float(count_tags.get(tag, 0))\n",
    "    mystartprob[tagid] = count_init_tags.get(tag, 0) / num_sentences\n",
    "    for word, wordid in word2id.items():\n",
    "        myemissionprob[tagid][wordid]= count_tags_to_words.get(tag, {}).get(word, 0) / floatCountTag\n",
    "    for tag2, tagid2 in tag2id.items():\n",
    "        mytransmat[tagid][tagid2]= count_tags_to_next_tags[tagid][tagid2] / sum_tags_to_next_tags[tagid]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize a HMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-20T21:46:54.238163Z",
     "iopub.status.busy": "2021-10-20T21:46:54.237926Z",
     "iopub.status.idle": "2021-10-20T21:46:54.243255Z",
     "shell.execute_reply": "2021-10-20T21:46:54.242324Z",
     "shell.execute_reply.started": "2021-10-20T21:46:54.238133Z"
    }
   },
   "outputs": [],
   "source": [
    "model = hmm.MultinomialHMM(n_components=len(tags), algorithm='viterbi', random_state=42)\n",
    "model.startprob_ = mystartprob\n",
    "model.transmat_ = mytransmat\n",
    "model.emissionprob_ = myemissionprob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As some words may never appear in the training set, we need to transform them into `UNKNOWN` first.\n",
    "Then we split `data_test` into `samples` & `lengths` and send them to HMM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-20T21:46:54.244806Z",
     "iopub.status.busy": "2021-10-20T21:46:54.244442Z",
     "iopub.status.idle": "2021-10-20T21:46:55.154971Z",
     "shell.execute_reply": "2021-10-20T21:46:55.153984Z",
     "shell.execute_reply.started": "2021-10-20T21:46:54.244773Z"
    }
   },
   "outputs": [],
   "source": [
    "data_test.loc[~data_test['Word'].isin(words), 'Word'] = 'UNKNOWN'\n",
    "word_test = list(data_test.Word)\n",
    "samples = []\n",
    "for i, val in enumerate(word_test):\n",
    "    samples.append([word2id[val]])\n",
    "    \n",
    "# TODO use panda solution\n",
    "lengths = []\n",
    "count = 0\n",
    "sentences = list(data_test.sentence)\n",
    "for i in range(len(sentences)) :\n",
    "    if (i > 0) and (sentences[i] == sentences[i - 1]):\n",
    "        count += 1\n",
    "    elif i > 0:\n",
    "        lengths.append(count)\n",
    "        count = 1\n",
    "    else:\n",
    "        count = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-20T21:46:55.156826Z",
     "iopub.status.busy": "2021-10-20T21:46:55.156418Z",
     "iopub.status.idle": "2021-10-20T21:49:56.298549Z",
     "shell.execute_reply": "2021-10-20T21:49:56.297746Z",
     "shell.execute_reply.started": "2021-10-20T21:46:55.156781Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 7, 32,  7, ..., 23, 41,  8], dtype=int32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This code is very slow\n",
    "pos_predict = model.predict(samples, lengths)\n",
    "pos_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-20T21:49:56.299781Z",
     "iopub.status.busy": "2021-10-20T21:49:56.299571Z",
     "iopub.status.idle": "2021-10-20T21:49:56.482558Z",
     "shell.execute_reply": "2021-10-20T21:49:56.481010Z",
     "shell.execute_reply.started": "2021-10-20T21:49:56.299756Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(345615, 345639, 345639, 345639)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags_test = list(data_test.POS)\n",
    "pos_test = np.zeros((len(tags_test), ), dtype=int)\n",
    "for i, val in enumerate(tags_test):\n",
    "    pos_test[i] = tag2id[val]\n",
    "len(pos_predict), len(pos_test), len(samples), len(word_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Somehow the output of HMM is in wrong size. Only use the shorter length to check the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-20T21:49:56.484981Z",
     "iopub.status.busy": "2021-10-20T21:49:56.484763Z",
     "iopub.status.idle": "2021-10-20T21:49:57.078323Z",
     "shell.execute_reply": "2021-10-20T21:49:57.077241Z",
     "shell.execute_reply.started": "2021-10-20T21:49:56.484954Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy is 0.9656062381551727\n",
      "The precision is 0.9657832270028688\n",
      "The recall is 0.9656062381551727\n",
      "The F1-Score is 0.9655716883723663\n"
     ]
    }
   ],
   "source": [
    "def reportTest(y_pred, y_test):\n",
    "    print(\"The accuracy is {}\".format(accuracy_score(y_test, y_pred))) \n",
    "    print(\"The precision is {}\".format(precision_score(y_test, y_pred, average='weighted'))) \n",
    "    print(\"The recall is {}\".format(recall_score(y_test, y_pred, average='weighted'))) \n",
    "    print(\"The F1-Score is {}\".format(f1_score(y_test, y_pred, average='weighted')))\n",
    "\n",
    "min_length = min(len(pos_predict), len(pos_test))\n",
    "\n",
    "reportTest(pos_predict[:min_length], pos_test[:min_length])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
